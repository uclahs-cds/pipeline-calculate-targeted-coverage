import nextflow.util.SysHelper

// Default inputs/parameters of the pipeline
params {
    max_cpus   = SysHelper.getAvailCpus()
    max_memory = SysHelper.getAvailMemory()

    min_cpus = 1
    min_memory = 1.MB

    gatk_command_mem_diff = 2.GB

    ucla_cds = true

    // default workflow params
    target_depth = false
    off_target_depth = true
    output_enriched_target_file = true

    // default run_depth_SAMtools params
    min_base_quality = 20
    min_mapping_quality = 20

    // default run_merge_BEDtools params
    merge_operation = "collapse"

    // default convert_depth_to_bed params
    save_all_dbSNP = false
    save_raw_target_bed = false

    // default run_CollectHsMetrics_picard params
    bait_interval_list = ''
    coverage_cap = 3000
    near_distance = 250

    // default run_slop_BEDtools params
    off_target_slop = 500
    dbSNP_slop = 150

    // default run_depth_filter params
    min_read_depth = 30

    // Docker images

    docker_container_registry = "ghcr.io/uclahs-cds"

    picard_version = "3.0.0"
    samtools_version = "1.16.1"
    pipeval_version = "4.0.0-rc.2"
    bedtools_version = "2.29.2"

    docker_image_picard = "${-> params.docker_container_registry}/picard:${picard_version}"
    docker_image_samtools = "${-> params.docker_container_registry}/samtools:${samtools_version}"
    docker_image_bedtools = "${-> params.docker_container_registry}/bedtools:${bedtools_version}"
    docker_image_validate = "${-> params.docker_container_registry}/pipeval:${params.pipeval_version}"
    }

// Process specific scope
process {
    // Process results are stored to local cache.
    // If pipeline is launched with the 'resume' option, existing cache results will be used when available
    // rather than re-executing processes
    cache = true

    // Forward process 'stdout' to shell terminal and, consequently, the log file
    echo = true
    executor = 'local'
    
    // Other directives or options that should apply for every process

    // total amount of resources avaible to the pipeline
    cpus = params.max_cpus
    memory = params.max_memory
}

// Enable Docker and configure docker-related options like user and group IDs
docker {
    enabled = true
    // Pass user's UID/GID and group IDs to Docker
    uid_and_gid = "-u \$(id -u):\$(id -g)"
    all_group_ids = "\$(for i in `id --real --groups`; do echo -n \"--group-add=\$i \"; done)"

    runOptions = "${uid_and_gid} ${all_group_ids}"
}
